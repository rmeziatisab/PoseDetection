{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/gist/rmeziatisab/20820a7c8cc667a1da44f22bcbcb7923/pose_detection_mediapipe.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6RCyy6NXnaO"
      },
      "source": [
        "# Human pose detection using MediaPipe\n",
        "MediaPipe is a powerful set of tools developed by Google, allowing to apply diversified tasks linked to computer vision (*eg* object detection, gesture recognition), text (*eg* text embedding) and audio data (audio classification). In this tutorial, I am going to show you how to use MediaPipe to detect pose landmarks in videos, following these steps:\n",
        "\n",
        "\n",
        "*   Import and install the necessary packages\n",
        "*   Download the MediaPipe pose landmark detection model\n",
        "*   Load an input video\n",
        "*   Extract the pose landmarks on each video frame, and visulize them\n",
        "*   Save the current frame alongside with the extracted pose landmarks\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fr1Xj4CRenAo"
      },
      "source": [
        "## Required packages\n",
        "\n",
        "To be able to follow this notebook, the following packages should be installed:\n",
        "\n",
        "\n",
        "*   MediaPipe\n",
        "*   OpenCV\n",
        "*   NumPy\n",
        "\n",
        "This tuto does not cover opencv and numpy installation. You can get MediaPipe installed using:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4kfsaiIgxVgE"
      },
      "outputs": [],
      "source": [
        "# Install MediaPipe\n",
        "!pip install -q mediapipe==0.10.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8EhkSxrhmMR"
      },
      "source": [
        "Let's begin with importing the above-mentioned packages:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tf5wqP6oPlEu"
      },
      "outputs": [],
      "source": [
        "# Import the required packages\n",
        "import cv2\n",
        "import numpy as np\n",
        "import mediapipe as mp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PY4NsBdipGwz"
      },
      "source": [
        "If you are running this notebook on Google Colab, and need to access files in you Google Drive, you will need to mount to Google Drive. This can be achieved using:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h3DJiQ1Myoia"
      },
      "outputs": [],
      "source": [
        "# Mount to Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vThhRVtyphvZ"
      },
      "source": [
        "## Pose landmark detection model\n",
        "\n",
        "The pose landmarker model proposed by MediaPipe allows to extract 33 body landmark 3D coordinates, estimate for each landmark whether it is visible in the input frame (meaning not hided by another body part or an object), and indicate the probabibility that it is present in the frame.\n",
        "\n",
        "You can download the pose landmarker model [here](https://developers.google.com/mediapipe/solutions/vision/pose_landmarker#models). For this tutorial, I will use the Full model. Here is another option to get the pose landmarker model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KPbDYfwnxbrf"
      },
      "outputs": [],
      "source": [
        "!wget -O pose_landmarker.task -q https://storage.googleapis.com/mediapipe-models/pose_landmarker/pose_landmarker_heavy/float16/1/pose_landmarker_full.task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOxiFggOxQvL"
      },
      "source": [
        "## Pose landmark visualization\n",
        "For body landmark visualization, I will be using the following function, developed by the MediaPipe authors. This function will be called to annotate the current frame with the detected body landmarks, as well as to draw connections between them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YurAWzW-RAbP"
      },
      "outputs": [],
      "source": [
        "# Use function allowing to draw pose landmarks on an image\n",
        "from mediapipe.python import solutions\n",
        "from mediapipe.framework.formats import landmark_pb2\n",
        "\n",
        "def draw_landmarks_on_image(rgb_image, detection_result):\n",
        "    pose_landmarks_list = detection_result.pose_landmarks\n",
        "    annotated_image = np.copy(rgb_image)\n",
        "\n",
        "    for idx in range(len(pose_landmarks_list)):\n",
        "        pose_landmarks = pose_landmarks_list[idx]\n",
        "\n",
        "        pose_landmarks_proto = landmark_pb2.NormalizedLandmarkList()\n",
        "        pose_landmarks_proto.landmark.extend([\n",
        "            landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y, z=landmark.z) for landmark in pose_landmarks\n",
        "        ])\n",
        "        solutions.drawing_utils.draw_landmarks(\n",
        "            annotated_image,\n",
        "            pose_landmarks_proto,\n",
        "            solutions.pose.POSE_CONNECTIONS,\n",
        "            solutions.drawing_styles.get_default_pose_landmarks_style()\n",
        "        )\n",
        "    return annotated_image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_6Nuekp3pGL"
      },
      "source": [
        "## Pose landmark extraction\n",
        "The first thing to do is load the video MediaPipe pose detection model will be applied to. The frame rate (expressed in frames per second, fps) and the frame dimensions are used to create the ouput video that will contain the input frames annotated with the detected pose landmarks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6hn3WxygRrrG"
      },
      "outputs": [],
      "source": [
        "# Read the input video and extract video information\n",
        "input_path = '/content/drive/My Drive/MOCA/Code/PoseEstimation/mini_vid.mp4'#'/content/drive/My Drive/MOCA/Data/YT-videos/cropped/sidehop00.mp4'\n",
        "cap = cv2.VideoCapture(input_path)\n",
        "\n",
        "if cap.isOpened() is False:\n",
        "    print('Video not found')\n",
        "else:\n",
        "  fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "  width = int(cap.get(cv2. CAP_PROP_FRAME_WIDTH))\n",
        "  height = int(cap.get(cv2. CAP_PROP_FRAME_HEIGHT))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n64hMoaeDg7C"
      },
      "source": [
        "We need to create a VideoWriter object to be able to save the output video. The video path - containing the video file extension - should be specified. The VideoWriter object constructor takes the path, the FourCC, the frame rate as well as the frame size. An `'MP4V'` FourCC corresponds to the MP4 format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e8xREqVoTPtJ"
      },
      "outputs": [],
      "source": [
        "# Create a VideoWriter object\n",
        "output_path = '/content/drive/My Drive/MOCA/Code/PoseEstimation/test.mp4'\n",
        "output_vid = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'MP4V'), fps, (width, height))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8grwJ4ObGPMF"
      },
      "source": [
        "Next, we set the options needed to create a PoseLandmarked object. Among the options, the downloaded model path and the video mode are indicated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0XF7ssg7PhSR"
      },
      "outputs": [],
      "source": [
        "# Set options for the PoseLandmarker object\n",
        "from mediapipe.tasks import python\n",
        "\n",
        "model_path = '/content/drive/My Drive/MOCA/Code/PoseEstimation/mediaPipe/pose_landmarker_full.task'\n",
        "\n",
        "BaseOptions = python.BaseOptions\n",
        "PoseLandmarker = python.vision.PoseLandmarker\n",
        "PoseLandmarkerOptions = python.vision.PoseLandmarkerOptions\n",
        "VisionRunningMode = python.vision.RunningMode\n",
        "options = PoseLandmarkerOptions(base_options=BaseOptions(model_asset_path=model_path),\n",
        "                                running_mode=VisionRunningMode.VIDEO)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyo39pOqJgQ2"
      },
      "source": [
        "Pose landmarks are detected for the input video frames: each frame is converted to an Image object that is passed to the `detect_for_video` method, with its corresponding frame timestamp obtained as:\n",
        "\n",
        "<center> $ frame\\,timestamp = \\frac{1000\\,*\\,frame\\,index}{frame\\,rate} $ </center>\n",
        "\n",
        "The frame index is an integer between $0$ and the total number of frames, describing the current frame number. The frame index can be obtained either with a counter variable incremented each time a new frame is read, or using the VideoCapure `get` method as follows:\n",
        "`int(cap.get(cv2.CAP_PROP_POS_FRAMES))`.\n",
        "\n",
        "Then, the `draw_landmarks_on_image` function is called to draw the detected body landmark positions and connections. I use the `cv2_imshow` function to display the annotated frames since I am using Google Colab. If you are using another environment, you can use `cv2.imshow`.\n",
        "\n",
        "Finally, the annotated frame is saved as an output video frame.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "nO-F4KeDU_h7"
      },
      "outputs": [],
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# Extract pose landmarks\n",
        "with PoseLandmarker.create_from_options(options) as landmarker:\n",
        "    frame_index = 0\n",
        "    while cap.isOpened():\n",
        "        hasFrame, image = cap.read()\n",
        "        if not hasFrame:\n",
        "            print('No more frames to read!')\n",
        "            break\n",
        "\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        numpy_frame_from_opencv = np.asarray(image)\n",
        "        mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=numpy_frame_from_opencv)\n",
        "        cv_frame_index = cap.get(cv2.CAP_PROP_POS_FRAMES)\n",
        "        frame_index += 1\n",
        "        frame_timestamp_ms = int(1000*frame_index / fps)\n",
        "        result = landmarker.detect_for_video(mp_image, frame_timestamp_ms)\n",
        "        annotated_image = draw_landmarks_on_image(mp_image.numpy_view(), result)\n",
        "        cv_image = cv2.cvtColor(annotated_image, cv2.COLOR_RGB2BGR)\n",
        "        cv2_imshow(cv_image)\n",
        "\n",
        "        if cv2.waitKey(5) & 0xFF == ord('q'):\n",
        "            break\n",
        "\n",
        "        output_vid.write(cv_image)\n",
        "\n",
        "    cap.release()\n",
        "    output_vid.release()\n",
        "\n",
        "cv2.destroyAllWindows()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNwcAvzZqats5qvdR6guPl/",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}